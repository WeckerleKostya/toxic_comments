# toxic_comments
# Описание проекта

* **Сфера деятельности**: Интернет-магазин

* **Цель**: построить модель поиска токсичных комментариев в разделах с описаниями товаров в новом сервисе.

* **Ключевые критерии качества модели**
    - Метрика F1 $>=$ 0.75

* **Задачи**:
    - Загрузка данных
    - Подготовка данных к построению моделей
    - Обучение моделей
    - Выбор наилучшей модели и выводы
    
**Основные инструменты:** `Python`, `Pandas`, `BERT`, `nltk`, `tf_idf`

**Направление деятельнсоти:** `Machine Learning`, `NLP`


## Описание данных:
* **`toxic`** — целевой признак.

## ОБЩИЙ ВЫВОД
* На основе полученных данных о содержании и токсичности комментариев были построены несколько классов моделей классификации комментариев по их "токсичности" **(логистическая регрессия, дерево решений, случайны лес, LGBMClassifier, CatBoostClassifier)**. В рамках перебора моделей использовались два принципа формирования прзнаков из текстовых данных **("мешок слов" и TF-IDF)**.
* Все модели оценивались с учетом значительного дисбаланса классов путем увеличения выборки (upsample метод).
* В ходе процесса лемматизации возникла ошибка.
    - ОШИБКА: "generator raised StopIteration"
    - Причина ошибки не утановлена
    - **Повторный запуск кода решил данную проблему** (поэтому лемматизация проведена в структуре `try ... except`
* Пришлось изменить способ лемматизации из-за ограничений JupyterHub. Время работы кода было увеличено.
* На основе полученных результатов оценивания моделии и тестирования **наилучшей оказалась модель логистической регрессии на основе признаков, свормированных по принципу TF-IDF**.
    - f1_score на обучающей выборке: **0.974** >0.75
    - f1_score на тестовой выборке: **0.755** > 0.75
